{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ds.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwFAU2kvoKOM12EOWULCMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spider-tronix/Dark-Sight/blob/colab_config/ds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODmQtbfY3jL4"
      },
      "source": [
        "#0M NAMO NARAYANA"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zyB3Hc_LKD0",
        "outputId": "f6ff15ca-a47e-42a9-d6eb-0943bccb5e26"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwhfO1no3oVi"
      },
      "source": [
        "import os\n",
        "os.listdir()\n",
        "try:\n",
        "  os.listdir().find('ds')\n",
        "  first_time = False\n",
        "except:\n",
        "  first_time = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Sxn2ae4K1Q"
      },
      "source": [
        "if (first_time):\n",
        "  !mkdir ds\n",
        "  os.chdir('ds')\n",
        "  os.listdir()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOQG834u4Qsj",
        "outputId": "ee02bf8b-6e6b-4d40-d1f4-5085c8793129"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5PELJuz4WqQ"
      },
      "source": [
        "target_datasetdir = '/content/drive/My Drive/Dataset/Dataset/'\n",
        "target_path = {\n",
        "    'dataset': '/content/drive/My Drive/Dataset/Dataset/', \n",
        "    'save': '/content/drive/My Drive/Dataset/Dataset/results/',\n",
        "    'checkpoint': '/content/drive/My Drive/Dataset/Colab/checkpoints/'\n",
        "    }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzHy_Imz4utU"
      },
      "source": [
        "if (first_time):\n",
        "  !git clone https://github.com/spider-tronix/Dark-Sight.git\n",
        "  os.chdir('./Dark-Sight')\n",
        "else:\n",
        "  !git pull\n",
        "print(os.listdir())\n",
        "!git checkout colab_config\n",
        "!git status\n",
        "!git log\n",
        "first_time = False\n",
        "print(os.listdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82Hm4dVaKigj",
        "outputId": "99d89db0-3bfd-4953-c30d-94d64fa036f0"
      },
      "source": [
        "!python ./data/darksight_dataloader.py"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-06 02:34:17.611926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "no. of datapoints 0\n",
            "Traceback (most recent call last):\n",
            "  File \"./data/darksight_dataloader.py\", line 241, in <module>\n",
            "    data = DarkSighDataLoader().load()\n",
            "  File \"./data/darksight_dataloader.py\", line 236, in load\n",
            "    dataloader = DataLoader(self.transformed_dataset, batch_size, shuffle=shuffle)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 262, in __init__\n",
            "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py\", line 104, in __init__\n",
            "    \"value, but got num_samples={}\".format(self.num_samples))\n",
            "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v4XbQ0KGsCq",
        "outputId": "c30573f0-703c-4c96-d338-21b438badb07"
      },
      "source": [
        "# OM NAMO NARAYANA\n",
        "\n",
        "\n",
        "\"\"\"run on base dir\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import errno\n",
        "\n",
        "sys.path.insert(1, \"./\")\n",
        "from DarkNet.models.sid_unet import sidUnet\n",
        "from DarkNet.models.lavi_unet import laviUnet\n",
        "from data.darksight_dataloader import DarkSighDataLoader\n",
        "\n",
        "\n",
        "\"\"\"Parameters\"\"\"\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "# set to True if thermal response included\n",
        "inc_therm = False\n",
        "load_weights = True\n",
        "\n",
        "# change the directory for required model\n",
        "checkpoint_dir = target_path['checkpoint']\n",
        "save_dir = target_path['save']\n",
        "raw_format = False\n",
        "model = laviUnet(inc_therm=inc_therm, raw_format=raw_format)\n",
        "plot_freq = 10\n",
        "error_freq = 5\n",
        "checkpoint_freq = 25\n",
        "dataset_dir = target_path['dataset']\n",
        "\n",
        "epochs = 500\n",
        "lr = 1e-4\n",
        "\n",
        "\n",
        "\"\"\"training code\"\"\"\n",
        "os.chdir(\"./\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "\n",
        "\"\"\"creating necessary directories\"\"\"\n",
        "\n",
        "if not raw_format:\n",
        "    save_dir = save_dir + \"jpg_format/\"\n",
        "    checkpoint_dir = checkpoint_dir + \"jpg_format/\"\n",
        "\n",
        "if not inc_therm:\n",
        "    save_dir = save_dir + \"without_therm/\"\n",
        "    checkpoint_dir = checkpoint_dir + \"without_therm/\"\n",
        "\n",
        "try:\n",
        "    os.makedirs(save_dir)\n",
        "    print(save_dir + \" created\")\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise\n",
        "\n",
        "try:\n",
        "    os.makedirs(checkpoint_dir)\n",
        "    print(checkpoint_dir + \" created\")\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise\n",
        "\n",
        "if not load_weights or os.listdir(checkpoint_dir) == []:\n",
        "    if load_weights:\n",
        "        print(\"no models saved\")\n",
        "    epoch_loaded = 0\n",
        "else:\n",
        "    checkpoint = torch.load(checkpoint_dir + os.listdir(checkpoint_dir)[-1])\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    epoch_loaded = checkpoint[\"epoch\"]\n",
        "    print(\"model loaded\")\n",
        "\n",
        "trainloader = DarkSighDataLoader(\n",
        "    inc_therm=inc_therm, raw_format=raw_format, dataset_dir=dataset_dir\n",
        ").load(batch_size=batch_size)\n",
        "\n",
        "for epoch in range(epoch_loaded, epoch_loaded + epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, gt = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        outputs = torch.transpose(outputs, 1, 3)\n",
        "        outputs = torch.transpose(outputs, 1, 2)\n",
        "        loss = criterion(outputs, gt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        print(\n",
        "                \"epoch: %d, iteration: %d runningloss: %.3f\" % (epoch + 1, i + 1, running_loss / 2)\n",
        "            )\n",
        "\n",
        "        \"\"\"saving results\"\"\"\n",
        "        if i % batch_size == 1 and epoch % error_freq == 1:\n",
        "            print(\n",
        "                \"epoch: %d, iteration: %d loss: %.3f\" % (epoch + 1, i + 1, running_loss / 2)\n",
        "            )\n",
        "            running_loss = 0.0\n",
        "            f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
        "            ax1.imshow(outputs.detach().numpy()[0])\n",
        "            ax2.imshow(gt.detach().numpy()[0])\n",
        "            ax3.imshow(\n",
        "                torch.transpose(torch.transpose(inputs.detach(), 1, 3), 1, 2).numpy()[\n",
        "                    0\n",
        "                ][:, :, :3]\n",
        "            )\n",
        "\n",
        "            plt.savefig(save_dir + \"epoch{}_index{}.png\".format(epoch + 1, i + 1))\n",
        "\n",
        "        \"\"\"saving checkpoints\"\"\"\n",
        "        if i % batch_size == 1 and epoch % checkpoint_freq == 1:\n",
        "\n",
        "            torch.save(\n",
        "                {\n",
        "                    \"epoch\": epoch,\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"loss\": loss,\n",
        "                },\n",
        "                checkpoint_dir + str(int(epoch / checkpoint_freq) + 1) + \".tar\",\n",
        "            )\n",
        "\n",
        "            print(\n",
        "                \"checkpoint %d_%d.tar saved.\"\n",
        "                % (int(epoch / checkpoint_freq) + 1, i + 1)\n",
        "            )\n",
        "\n",
        "\n",
        "print(\"Finished Training\")\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no models saved\n",
            "no. of datapoints 147\n",
            "data index 126\n",
            "sampled on 11 iteration\n",
            "data index 99\n",
            "sampled on 4 iteration\n",
            "epoch: 1, iteration: 1 runningloss: 0.211\n",
            "data index 42\n",
            "sampled on 5 iteration\n",
            "data index 134\n",
            "sampled on 2 iteration\n",
            "epoch: 1, iteration: 2 runningloss: 0.432\n",
            "data index 84\n",
            "sampled on 2 iteration\n",
            "data index 32\n",
            "sampled on 11 iteration\n",
            "epoch: 1, iteration: 3 runningloss: 0.657\n",
            "data index 60\n",
            "sampled on 1 iteration\n",
            "data index 133\n",
            "sampled on 6 iteration\n",
            "epoch: 1, iteration: 4 runningloss: 0.877\n",
            "data index 76\n",
            "sampled on 1 iteration\n",
            "data index 66\n",
            "sampled on 3 iteration\n",
            "epoch: 1, iteration: 5 runningloss: 1.093\n",
            "data index 132\n",
            "sampled on 8 iteration\n",
            "data index 6\n",
            "sampled on 1 iteration\n",
            "epoch: 1, iteration: 6 runningloss: 1.312\n",
            "data index 143\n",
            "sampled on 3 iteration\n",
            "data index 70\n",
            "sampled on 11 iteration\n",
            "epoch: 1, iteration: 7 runningloss: 1.531\n",
            "data index 11\n",
            "sampled on 1 iteration\n",
            "data index 13\n",
            "sampled on 1 iteration\n",
            "epoch: 1, iteration: 8 runningloss: 1.755\n",
            "data index 123\n",
            "sampled on 1 iteration\n",
            "data index 3\n",
            "sampled on 5 iteration\n",
            "epoch: 1, iteration: 9 runningloss: 1.975\n",
            "data index 130\n",
            "sampled on 1 iteration\n",
            "data index 68\n",
            "sampled on 1 iteration\n",
            "epoch: 1, iteration: 10 runningloss: 2.198\n",
            "data index 138\n",
            "sampled on 8 iteration\n",
            "data index 142\n",
            "sampled on 2 iteration\n",
            "epoch: 1, iteration: 11 runningloss: 2.421\n",
            "data index 18\n",
            "sampled on 2 iteration\n",
            "data index 35\n",
            "sampled on 1 iteration\n",
            "epoch: 1, iteration: 12 runningloss: 2.632\n",
            "data index 52\n",
            "sampled on 7 iteration\n",
            "data index 47\n",
            "sampled on 1 iteration\n",
            "epoch: 1, iteration: 13 runningloss: 2.848\n",
            "data index 5\n",
            "sampled on 1 iteration\n",
            "data index 21\n",
            "sampled on 3 iteration\n",
            "epoch: 1, iteration: 14 runningloss: 3.068\n",
            "data index 111\n",
            "sampled on 1 iteration\n",
            "data index 69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH_GgnkBH0em"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}